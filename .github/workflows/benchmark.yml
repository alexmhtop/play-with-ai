name: Performance Benchmarks

on:
  push:
    branches: [main]
    paths:
      - 'src/**'
      - 'tests/perf/**'
  pull_request:
    branches: [main]
    paths:
      - 'src/**'
      - 'tests/perf/**'
  workflow_dispatch:

permissions:
  contents: write
  pull-requests: write

jobs:
  benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    env:
      APP_DATABASE_URL: postgresql+psycopg://postgres:postgres@127.0.0.1:5432/postgres
    services:
      postgres:
        image: postgres:16-alpine
        ports:
          - 5432:5432
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: postgres
        options: >-
          --health-cmd="pg_isready -U postgres"
          --health-interval=10s
          --health-timeout=5s
          --health-retries=5
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-benchmark locust

      - name: Wait for Postgres
        run: |
          for i in {1..30}; do
            if pg_isready -h 127.0.0.1 -p 5432 -U postgres; then exit 0; fi
            sleep 2
          done
          exit 1

      - name: Run pytest benchmarks
        run: |
          pytest tests/perf/ --benchmark-only --benchmark-json=benchmark-results.json || true
        env:
          PYTHONPATH: ${{ github.workspace }}

      - name: Start application for load test
        run: |
          python -m uvicorn src.app:app --host 0.0.0.0 --port 8000 &
          sleep 5
          curl -sf http://localhost:8000/api/v1/health || exit 1
        env:
          PYTHONPATH: ${{ github.workspace }}

      - name: Run load test
        run: |
          cat > locustfile.py << 'EOF'
          from locust import HttpUser, task, between

          class BooksAPIUser(HttpUser):
              wait_time = between(0.1, 0.5)

              @task(10)
              def health_check(self):
                  self.client.get("/api/v1/health")
          EOF

          locust --headless --users 10 --spawn-rate 2 --run-time 30s \
            --host http://localhost:8000 \
            --csv=loadtest \
            --only-summary || true

      - name: Generate benchmark report
        run: |
          echo "## Performance Benchmark Results" > benchmark-report.md
          echo "" >> benchmark-report.md
          echo "### Load Test Summary" >> benchmark-report.md
          echo "" >> benchmark-report.md
          if [ -f loadtest_stats.csv ]; then
            echo '```' >> benchmark-report.md
            cat loadtest_stats.csv >> benchmark-report.md
            echo '```' >> benchmark-report.md
          fi
          echo "" >> benchmark-report.md
          echo "### Pytest Benchmarks" >> benchmark-report.md
          echo "" >> benchmark-report.md
          if [ -f benchmark-results.json ]; then
            echo '```json' >> benchmark-report.md
            cat benchmark-results.json | python -m json.tool >> benchmark-report.md || cat benchmark-results.json >> benchmark-report.md
            echo '```' >> benchmark-report.md
          fi

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: |
            benchmark-results.json
            benchmark-report.md
            loadtest_*.csv
          retention-days: 30

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v8
        with:
          script: |
            const fs = require('fs');
            let report = '## ðŸ“Š Performance Benchmark Results\n\n';

            if (fs.existsSync('benchmark-report.md')) {
              report = fs.readFileSync('benchmark-report.md', 'utf8');
            } else {
              report += 'Benchmark completed. See artifacts for details.';
            }

            // Find existing comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(c =>
              c.user.type === 'Bot' && c.body.includes('Performance Benchmark Results')
            );

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: report
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: report
              });
            }
